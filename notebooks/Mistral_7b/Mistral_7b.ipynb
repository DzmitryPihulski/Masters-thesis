{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cec6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "token_hf = os.environ['TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4614df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('data/Mistral/data.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc11a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "far_right_EN = \"\"\"Name: Mark Thompson\n",
    "Age: 52\n",
    "Sex: Male\n",
    "Nationality: American\n",
    "Political Group: Far-right conservative\n",
    "Outlook: Distrusts mainstream media, believes \"cancel culture\" suppresses free speech. Views criticism of Right policies as unfair attacks. Likely to downplay offensiveness in right-wing tweets, but label progressive tweets as 'aggressive' or 'divisive.'\"\"\"\n",
    "\n",
    "far_right_PL = \"\"\"Imię i nazwisko: Mark Thompson\n",
    "Wiek: 52\n",
    "Płeć: Mężczyzna\n",
    "Nacjonalność: Polak\n",
    "Grupa polityczna: Skrajnie prawicowy konserwatysta\n",
    "Poglądy: Nie ufa mediom głównego nurtu, uważa, że „kultura anulowania” tłumi wolność słowa. Postrzega krytykę polityki prawicowej jako niesprawiedliwe ataki. Prawdopodobnie bagatelizuje obraźliwe tweety prawicowe, ale określa tweety progresywne jako \"agresywne\" lub \"dzielące\".\"\"\"\n",
    "\n",
    "far_right_RU = \"\"\"Имя: Марк Томпсон\n",
    "Возраст: 52\n",
    "Пол: Мужчина\n",
    "Национальность: Русский\n",
    "Политическая группа: Крайне правый консерватор\n",
    "Взгляды: Не доверяет основным СМИ, считает, что «культура отмены» подавляет свободу слова. Рассматривает критику правой политики как несправедливые нападки. Склонен преуменьшать оскорбительность правых твитов, но называть прогрессивные твиты „агрессивными“ или «вызывающими разногласия».\"\"\"\n",
    "\n",
    "mod_cons_EN = \"\"\"Name: Linda Carter\n",
    "Age: 38\n",
    "Sex: Female\n",
    "Nationality: American\n",
    "Political Group: Moderate conservative\n",
    "Outlook: Values civility and tradition, dislikes extreme rhetoric from both sides. Might label overtly vulgar or conspiratorial tweets as offensive, even from the right.\"\"\"\n",
    "\n",
    "mod_cons_PL = \"\"\"Imię i nazwisko: Linda Carter\n",
    "Wiek: 38\n",
    "Płeć: Kobieta\n",
    "Nacjonalność: Polka\n",
    "Grupa polityczna: Umiarkowany konserwatysta\n",
    "Poglądy: Ceni uprzejmość i tradycję, nie lubi ekstremalnej retoryki z obu stron. Może uznać wulgarne lub konspiracyjne tweety za obraźliwe, nawet ze strony prawicy.\"\"\"\n",
    "\n",
    "mod_cons_RU = \"\"\"Имя: Линда Картер\n",
    "Возраст: 38\n",
    "Пол: Женщина\n",
    "Национальность: Русская\n",
    "Политическая группа: Умеренно-консервативная\n",
    "Взгляды: Ценит цивилизованность и традиции, не любит экстремальную риторику с обеих сторон. Может счесть оскорбительными откровенно вульгарные или конспирологические твиты, даже от правых.\"\"\"\n",
    "\n",
    "prog_left_EN = \"\"\"Name: Aisha Patel\n",
    "Age: 29\n",
    "Sex: Female\n",
    "Nationality: American\n",
    "Political Group: Progressive left\n",
    "Outlook: Highly sensitive to language targeting marginalized groups (racism, sexism). Likely to label tweets using terms like \"socialist\" pejoratively as offensive. Tolerant of aggressive progressive rhetoric if framed as social justice.\"\"\"\n",
    "\n",
    "prog_left_PL = \"\"\"Imię i nazwisko: Aisha Patel\n",
    "Wiek: 29 lat\n",
    "Płeć: Kobieta\n",
    "Nacjonalność: Polka\n",
    "Grupa polityczna: Postępowa lewica\n",
    "Perspektywy: Bardzo wrażliwa na język skierowany do grup marginalizowanych (rasizm, seksizm). Prawdopodobnie uzna za obraźliwe tweety używające pejoratywnych określeń takich jak „socjalista”. Toleruje agresywną, progresywną retorykę, jeśli jest oprawiona w ramy sprawiedliwości społecznej.\"\"\"\n",
    "\n",
    "prog_left_RU = \"\"\"Имя: Айша Патель\n",
    "Возраст: 29\n",
    "Пол: Женщина\n",
    "Национальность: Русская\n",
    "Политическая группа: Прогрессивные левые\n",
    "Взгляды: Очень чувствительна к языку, направленному на маргинализированные группы (расизм, сексизм). Скорее всего, сочтет оскорбительными твиты использующие уничижительные термины вроде «социалист». Терпимо относится к агрессивной прогрессивной риторике, если она оформлена как социальная справедливость.\"\"\"\n",
    "\n",
    "centrist_EN = \"\"\"Name: James Cooper\n",
    "Age: 45\n",
    "Sex: Male\n",
    "Nationality: American\n",
    "Political Group: Centrist/Independent\n",
    "Outlook: Pragmatic, dislikes partisan extremes. Harsher on tweets with misinformation or personal attacks, regardless of side. Less influenced by political loyalty; more likely to align annotations with factual accuracy and tone.\"\"\"\n",
    "\n",
    "centrist_PL = \"\"\"Imię i nazwisko: James Cooper\n",
    "Wiek: 45\n",
    "Płeć: Mężczyzna\n",
    "Nacjonalność: Polak\n",
    "Grupa polityczna: Centrysta/Niezależny\n",
    "Poglądy: Pragmatyczny, nie lubi skrajnych stronnictw. Ostrzejszy wobec tweetów zawierających dezinformacje lub ataki personalne, niezależnie od strony. Mniejszy wpływ lojalności politycznej; większe prawdopodobieństwo dostosowania adnotacji do faktycznej dokładności i tonu.\"\"\"\n",
    "\n",
    "centrist_RU = \"\"\"Имя: Джеймс Купер\n",
    "Возраст: 45\n",
    "Пол: Мужчина\n",
    "Национальность: Русский\n",
    "Политическая группа: Центрист/Независимый\n",
    "Взгляды: Прагматик, не любит крайностей в политике. Более жестко относится к твитам с дезинформацией или личными нападками, независимо от стороны. Менее подвержен влиянию политической лояльности; более склонен согласовывать аннотации с точностью фактов и тоном.\"\"\"\n",
    "system_prompt_EN = \"\"\"Give the answer as only 0(not offensive) or 1(offensive). Do not include anything else in your response.\"\"\"\n",
    "\n",
    "user_text_EN = \"\"\"You are a person of this personality: \n",
    "{personality}\n",
    "\n",
    "How this person will label that statement (offensive or not):\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "system_prompt_PL = \"\"\"Podaj odpowiedź jako 0 (nie obraźliwe) lub 1 (obraźliwe). Nie uwzględniaj niczego innego w swojej odpowiedzi.\"\"\"\n",
    "\n",
    "user_text_PL = \"\"\"Jesteś osobą o tej osobowości:\n",
    "{personality}\n",
    "\n",
    "Jak ta osoba określi to stwierdzenie (obraźliwe lub nie): Jesteś osobą o tej osobowości:\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "system_prompt_RU = \"\"\"Укажите в ответе только 0 (не оскорбительно) или 1 (оскорбительно). Не указывай в ответе ничего другого.\"\"\"\n",
    "\n",
    "user_text_RU = \"\"\"Ты - человек этой личности:\n",
    "{personality}\n",
    "\n",
    "Как этот человек обозначит это высказывание (оскорбительное или нет):\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "# Define personalities\n",
    "personalities = {\n",
    "    \"far_right_EN\": far_right_EN,\n",
    "    \"far_right_PL\": far_right_PL,\n",
    "    \"far_right_RU\": far_right_RU,\n",
    "    \"mod_cons_EN\": mod_cons_EN,\n",
    "    \"mod_cons_PL\": mod_cons_PL,\n",
    "    \"mod_cons_RU\": mod_cons_RU,\n",
    "    \"prog_left_EN\": prog_left_EN,\n",
    "    \"prog_left_PL\": prog_left_PL,\n",
    "    \"prog_left_RU\": prog_left_RU,\n",
    "    \"centrist_EN\": centrist_EN,\n",
    "    \"centrist_PL\": centrist_PL,\n",
    "    \"centrist_RU\": centrist_RU,\n",
    "}\n",
    "\n",
    "sys_prompt_mapping = {\n",
    "    \"system_prompt_EN\": system_prompt_EN,\n",
    "    \"system_prompt_PL\": system_prompt_PL,\n",
    "    \"system_prompt_RU\": system_prompt_RU\n",
    "}\n",
    "\n",
    "user_prompt_mapping = {\n",
    "    \"user_text_EN\": user_text_EN,\n",
    "    \"user_text_PL\": user_text_PL,\n",
    "    \"user_text_RU\": user_text_RU\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9d6a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [01:19<00:00, 26.42s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", device_map='auto', token=token_hf)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", device_map='auto', token=token_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5efb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to get token probabilities from Mistral model in batches\n",
    "def get_mistral_probabilities(system_prompts, user_prompts, tokenizer, model, temperature=1):\n",
    "    \"\"\"\n",
    "    Process multiple conversations in parallel with the Mistral model,\n",
    "    returning probabilities for tokens \"0\" and \"1\".\n",
    "    \n",
    "    Args:\n",
    "        system_prompts: List of system prompts\n",
    "        user_prompts: List of user prompts\n",
    "        tokenizer: The tokenizer for the model\n",
    "        model: The Mistral model\n",
    "        temperature: Sampling temperature\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        A list of probability strings in format \"prob_for_1, prob_for_0\"\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(system_prompts) != len(user_prompts):\n",
    "        raise ValueError(\"Number of system prompts must match number of user prompts\")\n",
    "\n",
    "    \n",
    "    # Token IDs for Mistral (based on tokenizer results)\n",
    "    # Mistral uses two tokens for digits: a prefix token followed by the actual digit token\n",
    "    prefix_token_id = 29473  # The prefix token that appears before digits\n",
    "    token_id_0 = 29502      # Direct lookup token ID for \"0\"\n",
    "    token_id_1 = 29508      # Direct lookup token ID for \"1\"\n",
    "    \n",
    "    # Prepare all conversations\n",
    "    conversations = [\n",
    "        [{\"role\": \"system\", \"content\": system_prompt},\n",
    "         {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        for system_prompt, user_prompt in zip(system_prompts, user_prompts)\n",
    "    ]\n",
    "    \n",
    "    # Process each conversation\n",
    "    probabilities = []\n",
    "    \n",
    "    for conversation in conversations:\n",
    "        # Apply chat template\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            conversation, \n",
    "            add_generation_prompt=True, \n",
    "            tokenize=True,\n",
    "            return_dict=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Need to handle the two-token sequence for digits\n",
    "        # First, get the initial token prediction\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(inputs[\"input_ids\"])\n",
    "            first_token_logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                first_token_logits = first_token_logits / temperature\n",
    "                \n",
    "            # Convert logits to probabilities using softmax\n",
    "            first_token_probs = torch.nn.functional.softmax(first_token_logits, dim=-1)\n",
    "            \n",
    "            # Check probability of the prefix token\n",
    "            prefix_prob = first_token_probs[prefix_token_id].item()\n",
    "            \n",
    "            # If prefix token is likely, we need to get the probabilities for the second token\n",
    "            # Create a new input with the prefix token appended\n",
    "            new_input_ids = torch.cat([inputs[\"input_ids\"], torch.tensor([[prefix_token_id]]).to(model.device)], dim=1)\n",
    "            \n",
    "            # Get predictions for the second token after the prefix\n",
    "            outputs_second = model(new_input_ids)\n",
    "            second_token_logits = outputs_second.logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                second_token_logits = second_token_logits / temperature\n",
    "                \n",
    "            # Convert to probabilities\n",
    "            second_token_probs = torch.nn.functional.softmax(second_token_logits, dim=-1)\n",
    "            \n",
    "            # Get probabilities for the digit tokens after the prefix\n",
    "            prob_0 = second_token_probs[token_id_0].item()\n",
    "            prob_1 = second_token_probs[token_id_1].item()\n",
    "            \n",
    "            # Adjust probabilities by the probability of the prefix token\n",
    "            prob_0 = prefix_prob * prob_0\n",
    "            prob_1 = prefix_prob * prob_1\n",
    "        \n",
    "        # Format as requested: \"prob_for_1, prob_for_0\"\n",
    "        probability_str = f\"{prob_1:.4f}, {prob_0:.4f}\"\n",
    "        probabilities.append(probability_str)\n",
    "    \n",
    "    return probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5c3b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data using the batch-enabled Mistral probability function\n",
    "def process_data_with_mistral(data, personalities, user_prompt_mapping, sys_prompt_mapping, tokenizer, model, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process data using the get_mistral_probabilities function in batches.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the data\n",
    "        personalities: Dictionary of personality prompts\n",
    "        user_prompt_mapping: Dictionary mapping language codes to user prompt templates\n",
    "        sys_prompt_mapping: Dictionary mapping language codes to system prompts\n",
    "        tokenizer: The tokenizer for the model\n",
    "        model: The Mistral model\n",
    "        batch_size: Number of requests to batch together\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with Mistral token probabilities\n",
    "    \"\"\"\n",
    "    # Ensure all columns exist with appropriate types\n",
    "    for language_code in ['EN', 'PL', 'RU']:\n",
    "        for personality_type in [\"far_right\", \"mod_cons\", \"prog_left\", \"centrist\"]:\n",
    "            answer_col = f\"Mistral_{personality_type}_{language_code}_answer\"\n",
    "            \n",
    "            # Make sure answer column exists\n",
    "            if answer_col not in data.columns:\n",
    "                data[answer_col] = np.nan\n",
    "    \n",
    "    # Create a list to track all items that need processing\n",
    "    items_to_process = []\n",
    "    \n",
    "    # First scan to identify all items needing processing\n",
    "    for i, row in data.iterrows():\n",
    "        for language_code in ['EN', 'PL', 'RU']:\n",
    "            for personality_type in [\"far_right\", \"mod_cons\", \"prog_left\", \"centrist\"]:\n",
    "                personality_key = f\"{personality_type}_{language_code}\"\n",
    "                answer_col = f\"Mistral_{personality_type}_{language_code}_answer\"\n",
    "                \n",
    "                # Only process if answer column is empty\n",
    "                if pd.isna(data.loc[i, answer_col]):\n",
    "                    tweet = row[f\"Text_{language_code}\"]\n",
    "                    personality = personalities[personality_key]\n",
    "                    prompt = user_prompt_mapping[f\"user_text_{language_code}\"].format(personality=personality, tweet=tweet)\n",
    "                    \n",
    "                    items_to_process.append({\n",
    "                        'row_index': i,\n",
    "                        'personality_type': personality_type,\n",
    "                        'language_code': language_code,\n",
    "                        'system_prompt': sys_prompt_mapping[f\"system_prompt_{language_code}\"],\n",
    "                        'user_prompt': prompt\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(items_to_process)} items to process\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, len(items_to_process), batch_size)):\n",
    "        batch = items_to_process[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        # Extract system prompts and user prompts for the batch\n",
    "        system_prompts = [item['system_prompt'] for item in batch]\n",
    "        user_prompts = [item['user_prompt'] for item in batch]\n",
    "        \n",
    "        # Get probabilities\n",
    "        try:\n",
    "            probability_strs = get_mistral_probabilities(\n",
    "                system_prompts, \n",
    "                user_prompts,\n",
    "                tokenizer,\n",
    "                model,\n",
    "                temperature=1,\n",
    "            )\n",
    "            \n",
    "            # Update dataframe with results\n",
    "            for i, (item, prob_str) in enumerate(zip(batch, probability_strs)):\n",
    "                row_idx = item['row_index']\n",
    "                \n",
    "                # Store the probability string in the answer column\n",
    "                answer_col = f\"Mistral_{item['personality_type']}_{item['language_code']}_answer\"\n",
    "                data.loc[row_idx, answer_col] = prob_str\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {batch_start}: {e}\")\n",
    "        \n",
    "        # Save progress periodically with error handling\n",
    "        if batch_start % (batch_size * 5) == 0 or batch_start + batch_size >= len(items_to_process):\n",
    "            try:\n",
    "                # Create a copy of the dataframe with consistent types for saving\n",
    "                save_df = data.copy()\n",
    "                save_df.to_parquet(\"data/Mistral/data_mistral_progress.parquet\")\n",
    "                print(f\"Saved progress after processing {batch_start + len(batch)} items\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving data: {e}\")\n",
    "                # Try alternate save approach\n",
    "                try:\n",
    "                    # Save as CSV if parquet fails\n",
    "                    data.to_csv(\"data/Mistral/data_mistral_progress_backup.tsv\", sep='\\t')\n",
    "                    print(\"Saved backup as TSV instead\")\n",
    "                except:\n",
    "                    print(\"Unable to save progress in any format. Continuing.\")\n",
    "            \n",
    "        # Optional: Add a small delay between batches to prevent overloading\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89fc5806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 items to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:37<11:45, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [03:49<09:03, 38.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 1080 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [06:55<05:38, 37.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 1980 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [10:04<02:30, 37.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 2880 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:30<00:00, 37.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 3600 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = process_data_with_mistral(\n",
    "    data, \n",
    "    personalities, \n",
    "    user_prompt_mapping, \n",
    "    sys_prompt_mapping,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    batch_size=180  # Adjust based on GPU memory\n",
    ")\n",
    "try:\n",
    "    # Create a copy of the dataframe with consistent types for saving\n",
    "    save_df = results.copy()\n",
    "    save_df.to_parquet(\"data/Mistral/data_mistral_complete.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "    # Try alternate save approach\n",
    "    try:\n",
    "        # Save as CSV if parquet fails\n",
    "        results.to_csv(\"data/Mistral/data_mistral_complete_backup.tsv\", sep='\\t')\n",
    "        print(\"Saved backup as TSV instead\")\n",
    "    except:\n",
    "        print(\"Unable to save complete in any format. Continuing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b03696b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = pd.read_parquet('data/Mistral/data_mistral_complete.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79b7782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mistral_far_right_EN_answer</th>\n",
       "      <th>Mistral_far_right_PL_answer</th>\n",
       "      <th>Mistral_far_right_RU_answer</th>\n",
       "      <th>Mistral_mod_cons_EN_answer</th>\n",
       "      <th>Mistral_mod_cons_PL_answer</th>\n",
       "      <th>Mistral_mod_cons_RU_answer</th>\n",
       "      <th>Mistral_prog_left_EN_answer</th>\n",
       "      <th>Mistral_prog_left_PL_answer</th>\n",
       "      <th>Mistral_prog_left_RU_answer</th>\n",
       "      <th>Mistral_centrist_EN_answer</th>\n",
       "      <th>Mistral_centrist_PL_answer</th>\n",
       "      <th>Mistral_centrist_RU_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0051, 0.9943</td>\n",
       "      <td>0.6085, 0.3909</td>\n",
       "      <td>0.9678, 0.0305</td>\n",
       "      <td>0.0154, 0.9845</td>\n",
       "      <td>0.9975, 0.0020</td>\n",
       "      <td>0.1490, 0.8495</td>\n",
       "      <td>0.0002, 0.9997</td>\n",
       "      <td>0.0075, 0.9912</td>\n",
       "      <td>0.0071, 0.9909</td>\n",
       "      <td>0.0001, 0.9999</td>\n",
       "      <td>0.9208, 0.0785</td>\n",
       "      <td>0.4667, 0.5320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0626, 0.9368</td>\n",
       "      <td>0.9946, 0.0044</td>\n",
       "      <td>0.9858, 0.0123</td>\n",
       "      <td>0.9946, 0.0048</td>\n",
       "      <td>0.9997, 0.0000</td>\n",
       "      <td>0.9985, 0.0003</td>\n",
       "      <td>0.9985, 0.0010</td>\n",
       "      <td>0.9991, 0.0000</td>\n",
       "      <td>0.9965, 0.0003</td>\n",
       "      <td>0.0755, 0.9245</td>\n",
       "      <td>0.9989, 0.0005</td>\n",
       "      <td>0.9963, 0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8103, 0.1890</td>\n",
       "      <td>0.4483, 0.5515</td>\n",
       "      <td>0.9807, 0.0186</td>\n",
       "      <td>0.9991, 0.0004</td>\n",
       "      <td>0.9678, 0.0321</td>\n",
       "      <td>0.9993, 0.0001</td>\n",
       "      <td>0.9995, 0.0000</td>\n",
       "      <td>0.9989, 0.0008</td>\n",
       "      <td>0.9985, 0.0002</td>\n",
       "      <td>0.8796, 0.1203</td>\n",
       "      <td>0.8923, 0.1076</td>\n",
       "      <td>0.9975, 0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0214, 0.9783</td>\n",
       "      <td>0.0144, 0.9850</td>\n",
       "      <td>0.0566, 0.9416</td>\n",
       "      <td>0.1162, 0.8835</td>\n",
       "      <td>0.0060, 0.9935</td>\n",
       "      <td>0.9830, 0.0152</td>\n",
       "      <td>0.9993, 0.0002</td>\n",
       "      <td>0.0273, 0.9720</td>\n",
       "      <td>0.8883, 0.1082</td>\n",
       "      <td>0.0003, 0.9997</td>\n",
       "      <td>0.0046, 0.9948</td>\n",
       "      <td>0.0911, 0.9076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6956, 0.3039</td>\n",
       "      <td>0.9365, 0.0624</td>\n",
       "      <td>0.0924, 0.9055</td>\n",
       "      <td>0.9767, 0.0187</td>\n",
       "      <td>0.9978, 0.0015</td>\n",
       "      <td>0.9926, 0.0055</td>\n",
       "      <td>0.9937, 0.0004</td>\n",
       "      <td>0.9987, 0.0003</td>\n",
       "      <td>0.9919, 0.0050</td>\n",
       "      <td>0.9903, 0.0083</td>\n",
       "      <td>0.9821, 0.0174</td>\n",
       "      <td>0.9245, 0.0741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.9733, 0.0265</td>\n",
       "      <td>0.0628, 0.9364</td>\n",
       "      <td>0.9786, 0.0206</td>\n",
       "      <td>0.9999, 0.0000</td>\n",
       "      <td>0.2345, 0.7652</td>\n",
       "      <td>0.9992, 0.0001</td>\n",
       "      <td>0.9998, 0.0000</td>\n",
       "      <td>0.9926, 0.0068</td>\n",
       "      <td>0.9987, 0.0001</td>\n",
       "      <td>0.9979, 0.0020</td>\n",
       "      <td>0.0019, 0.9977</td>\n",
       "      <td>0.9988, 0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.0015, 0.9981</td>\n",
       "      <td>0.0500, 0.9497</td>\n",
       "      <td>0.0768, 0.9221</td>\n",
       "      <td>0.0000, 1.0000</td>\n",
       "      <td>0.0013, 0.9985</td>\n",
       "      <td>0.0211, 0.9778</td>\n",
       "      <td>0.0003, 0.9994</td>\n",
       "      <td>0.0379, 0.9617</td>\n",
       "      <td>0.3946, 0.6035</td>\n",
       "      <td>0.0000, 0.9999</td>\n",
       "      <td>0.0010, 0.9988</td>\n",
       "      <td>0.0345, 0.9649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.9604, 0.0390</td>\n",
       "      <td>0.8664, 0.1333</td>\n",
       "      <td>0.9791, 0.0199</td>\n",
       "      <td>0.9998, 0.0000</td>\n",
       "      <td>0.9992, 0.0006</td>\n",
       "      <td>0.9990, 0.0001</td>\n",
       "      <td>0.9933, 0.0057</td>\n",
       "      <td>0.0686, 0.9310</td>\n",
       "      <td>0.9982, 0.0002</td>\n",
       "      <td>0.9990, 0.0009</td>\n",
       "      <td>0.9749, 0.0249</td>\n",
       "      <td>0.9988, 0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.7329, 0.2668</td>\n",
       "      <td>0.2010, 0.7984</td>\n",
       "      <td>0.7328, 0.2659</td>\n",
       "      <td>0.9998, 0.0000</td>\n",
       "      <td>0.4948, 0.5046</td>\n",
       "      <td>0.9944, 0.0038</td>\n",
       "      <td>0.9997, 0.0000</td>\n",
       "      <td>0.9925, 0.0066</td>\n",
       "      <td>0.9921, 0.0037</td>\n",
       "      <td>0.7526, 0.2474</td>\n",
       "      <td>0.6714, 0.3281</td>\n",
       "      <td>0.9929, 0.0062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.8708, 0.1291</td>\n",
       "      <td>0.9929, 0.0065</td>\n",
       "      <td>0.9962, 0.0028</td>\n",
       "      <td>0.9996, 0.0004</td>\n",
       "      <td>0.9995, 0.0002</td>\n",
       "      <td>0.9990, 0.0001</td>\n",
       "      <td>0.9999, 0.0000</td>\n",
       "      <td>0.9993, 0.0002</td>\n",
       "      <td>0.9973, 0.0008</td>\n",
       "      <td>0.9842, 0.0157</td>\n",
       "      <td>0.9963, 0.0031</td>\n",
       "      <td>0.9988, 0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mistral_far_right_EN_answer Mistral_far_right_PL_answer  \\\n",
       "1                0.0051, 0.9943              0.6085, 0.3909   \n",
       "2                0.0626, 0.9368              0.9946, 0.0044   \n",
       "3                0.8103, 0.1890              0.4483, 0.5515   \n",
       "4                0.0214, 0.9783              0.0144, 0.9850   \n",
       "5                0.6956, 0.3039              0.9365, 0.0624   \n",
       "..                          ...                         ...   \n",
       "296              0.9733, 0.0265              0.0628, 0.9364   \n",
       "297              0.0015, 0.9981              0.0500, 0.9497   \n",
       "298              0.9604, 0.0390              0.8664, 0.1333   \n",
       "299              0.7329, 0.2668              0.2010, 0.7984   \n",
       "300              0.8708, 0.1291              0.9929, 0.0065   \n",
       "\n",
       "    Mistral_far_right_RU_answer Mistral_mod_cons_EN_answer  \\\n",
       "1                0.9678, 0.0305             0.0154, 0.9845   \n",
       "2                0.9858, 0.0123             0.9946, 0.0048   \n",
       "3                0.9807, 0.0186             0.9991, 0.0004   \n",
       "4                0.0566, 0.9416             0.1162, 0.8835   \n",
       "5                0.0924, 0.9055             0.9767, 0.0187   \n",
       "..                          ...                        ...   \n",
       "296              0.9786, 0.0206             0.9999, 0.0000   \n",
       "297              0.0768, 0.9221             0.0000, 1.0000   \n",
       "298              0.9791, 0.0199             0.9998, 0.0000   \n",
       "299              0.7328, 0.2659             0.9998, 0.0000   \n",
       "300              0.9962, 0.0028             0.9996, 0.0004   \n",
       "\n",
       "    Mistral_mod_cons_PL_answer Mistral_mod_cons_RU_answer  \\\n",
       "1               0.9975, 0.0020             0.1490, 0.8495   \n",
       "2               0.9997, 0.0000             0.9985, 0.0003   \n",
       "3               0.9678, 0.0321             0.9993, 0.0001   \n",
       "4               0.0060, 0.9935             0.9830, 0.0152   \n",
       "5               0.9978, 0.0015             0.9926, 0.0055   \n",
       "..                         ...                        ...   \n",
       "296             0.2345, 0.7652             0.9992, 0.0001   \n",
       "297             0.0013, 0.9985             0.0211, 0.9778   \n",
       "298             0.9992, 0.0006             0.9990, 0.0001   \n",
       "299             0.4948, 0.5046             0.9944, 0.0038   \n",
       "300             0.9995, 0.0002             0.9990, 0.0001   \n",
       "\n",
       "    Mistral_prog_left_EN_answer Mistral_prog_left_PL_answer  \\\n",
       "1                0.0002, 0.9997              0.0075, 0.9912   \n",
       "2                0.9985, 0.0010              0.9991, 0.0000   \n",
       "3                0.9995, 0.0000              0.9989, 0.0008   \n",
       "4                0.9993, 0.0002              0.0273, 0.9720   \n",
       "5                0.9937, 0.0004              0.9987, 0.0003   \n",
       "..                          ...                         ...   \n",
       "296              0.9998, 0.0000              0.9926, 0.0068   \n",
       "297              0.0003, 0.9994              0.0379, 0.9617   \n",
       "298              0.9933, 0.0057              0.0686, 0.9310   \n",
       "299              0.9997, 0.0000              0.9925, 0.0066   \n",
       "300              0.9999, 0.0000              0.9993, 0.0002   \n",
       "\n",
       "    Mistral_prog_left_RU_answer Mistral_centrist_EN_answer  \\\n",
       "1                0.0071, 0.9909             0.0001, 0.9999   \n",
       "2                0.9965, 0.0003             0.0755, 0.9245   \n",
       "3                0.9985, 0.0002             0.8796, 0.1203   \n",
       "4                0.8883, 0.1082             0.0003, 0.9997   \n",
       "5                0.9919, 0.0050             0.9903, 0.0083   \n",
       "..                          ...                        ...   \n",
       "296              0.9987, 0.0001             0.9979, 0.0020   \n",
       "297              0.3946, 0.6035             0.0000, 0.9999   \n",
       "298              0.9982, 0.0002             0.9990, 0.0009   \n",
       "299              0.9921, 0.0037             0.7526, 0.2474   \n",
       "300              0.9973, 0.0008             0.9842, 0.0157   \n",
       "\n",
       "    Mistral_centrist_PL_answer Mistral_centrist_RU_answer  \n",
       "1               0.9208, 0.0785             0.4667, 0.5320  \n",
       "2               0.9989, 0.0005             0.9963, 0.0027  \n",
       "3               0.8923, 0.1076             0.9975, 0.0019  \n",
       "4               0.0046, 0.9948             0.0911, 0.9076  \n",
       "5               0.9821, 0.0174             0.9245, 0.0741  \n",
       "..                         ...                        ...  \n",
       "296             0.0019, 0.9977             0.9988, 0.0006  \n",
       "297             0.0010, 0.9988             0.0345, 0.9649  \n",
       "298             0.9749, 0.0249             0.9988, 0.0005  \n",
       "299             0.6714, 0.3281             0.9929, 0.0062  \n",
       "300             0.9963, 0.0031             0.9988, 0.0005  \n",
       "\n",
       "[300 rows x 12 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3530056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Mean P(1)  Mean P(0)  Min P(1)  Max P(1)  \\\n",
      "Mistral_far_right_EN_answer   0.496318   0.503264    0.0000    0.9999   \n",
      "Mistral_far_right_PL_answer   0.733301   0.265266    0.0000    0.9997   \n",
      "Mistral_far_right_RU_answer   0.703199   0.295544    0.0001    0.9992   \n",
      "Mistral_mod_cons_EN_answer    0.668127   0.331598    0.0000    1.0000   \n",
      "Mistral_mod_cons_PL_answer    0.841022   0.156094    0.0000    0.9999   \n",
      "Mistral_mod_cons_RU_answer    0.835353   0.163485    0.0006    0.9995   \n",
      "Mistral_prog_left_EN_answer   0.663782   0.335623    0.0000    1.0000   \n",
      "Mistral_prog_left_PL_answer   0.787556   0.208964    0.0000    0.9998   \n",
      "Mistral_prog_left_RU_answer   0.831067   0.166946    0.0004    0.9991   \n",
      "Mistral_centrist_EN_answer    0.515013   0.484904    0.0000    1.0000   \n",
      "Mistral_centrist_PL_answer    0.747076   0.252523    0.0000    0.9997   \n",
      "Mistral_centrist_RU_answer    0.811684   0.187469    0.0004    0.9994   \n",
      "\n",
      "                             Std P(1)  Sanity Check (P1 + P0 ≈ 1)  \n",
      "Mistral_far_right_EN_answer  0.452294                       False  \n",
      "Mistral_far_right_PL_answer  0.369841                       False  \n",
      "Mistral_far_right_RU_answer  0.399365                       False  \n",
      "Mistral_mod_cons_EN_answer   0.453054                       False  \n",
      "Mistral_mod_cons_PL_answer   0.336869                       False  \n",
      "Mistral_mod_cons_RU_answer   0.338723                       False  \n",
      "Mistral_prog_left_EN_answer  0.451347                       False  \n",
      "Mistral_prog_left_PL_answer  0.375438                       False  \n",
      "Mistral_prog_left_RU_answer  0.334530                       False  \n",
      "Mistral_centrist_EN_answer   0.462124                       False  \n",
      "Mistral_centrist_PL_answer   0.398949                       False  \n",
      "Mistral_centrist_RU_answer   0.343583                       False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6436/2209124122.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  P1 = complete.iloc[:, 4:].applymap(lambda x: float(x.split(',')[0].strip()))\n",
      "/tmp/ipykernel_6436/2209124122.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  P0 = complete.iloc[:, 4:].applymap(lambda x: float(x.split(',')[1].strip()))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split into two new DataFrames: P1 (prob. of 1) and P0 (prob. of 0)\n",
    "P1 = complete.iloc[:, 4:].applymap(lambda x: float(x.split(',')[0].strip()))\n",
    "P0 = complete.iloc[:, 4:].applymap(lambda x: float(x.split(',')[1].strip()))\n",
    "\n",
    "# Step 2: Analyze each column\n",
    "analysis = pd.DataFrame(index=complete.iloc[:, 4:].columns)\n",
    "analysis['Mean P(1)'] = P1.mean()\n",
    "analysis['Mean P(0)'] = P0.mean()\n",
    "analysis['Min P(1)'] = P1.min()\n",
    "analysis['Max P(1)'] = P1.max()\n",
    "analysis['Std P(1)'] = P1.std()\n",
    "analysis['Sanity Check (P1 + P0 ≈ 1)'] = (P1 + P0).apply(lambda col: col.sub(1).abs().max() < 1e-6)\n",
    "\n",
    "print(analysis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
