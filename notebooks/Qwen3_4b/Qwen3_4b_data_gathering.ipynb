{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fc2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "token_hf = os.environ['TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ddef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('data/Qwen/data.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eecc5332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 1 to 300\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Text_EN                   300 non-null    object \n",
      " 1   Text_PL                   300 non-null    object \n",
      " 2   Text_RU                   300 non-null    object \n",
      " 3   Translation_quality       300 non-null    float64\n",
      " 4   Qwen_far_right_EN_answer  0 non-null      float64\n",
      " 5   Qwen_far_right_PL_answer  0 non-null      float64\n",
      " 6   Qwen_far_right_RU_answer  0 non-null      float64\n",
      " 7   Qwen_mod_cons_EN_answer   0 non-null      float64\n",
      " 8   Qwen_mod_cons_PL_answer   0 non-null      float64\n",
      " 9   Qwen_mod_cons_RU_answer   0 non-null      float64\n",
      " 10  Qwen_prog_left_EN_answer  0 non-null      float64\n",
      " 11  Qwen_prog_left_PL_answer  0 non-null      float64\n",
      " 12  Qwen_prog_left_RU_answer  0 non-null      float64\n",
      " 13  Qwen_centrist_EN_answer   0 non-null      float64\n",
      " 14  Qwen_centrist_PL_answer   0 non-null      float64\n",
      " 15  Qwen_centrist_RU_answer   0 non-null      float64\n",
      "dtypes: float64(13), object(3)\n",
      "memory usage: 37.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a202862",
   "metadata": {},
   "outputs": [],
   "source": [
    "far_right_EN = \"\"\"Name: Mark Thompson\n",
    "Age: 52\n",
    "Sex: Male\n",
    "Nationality: American\n",
    "Political Group: Far-right conservative\n",
    "Outlook: Distrusts mainstream media, believes \"cancel culture\" suppresses free speech. Views criticism of Right policies as unfair attacks. Likely to downplay offensiveness in right-wing tweets, but label progressive tweets as 'aggressive' or 'divisive.'\"\"\"\n",
    "\n",
    "far_right_PL = \"\"\"Imię i nazwisko: Mark Thompson\n",
    "Wiek: 52\n",
    "Płeć: Mężczyzna\n",
    "Nacjonalność: Polak\n",
    "Grupa polityczna: Skrajnie prawicowy konserwatysta\n",
    "Poglądy: Nie ufa mediom głównego nurtu, uważa, że „kultura anulowania” tłumi wolność słowa. Postrzega krytykę polityki prawicowej jako niesprawiedliwe ataki. Prawdopodobnie bagatelizuje obraźliwe tweety prawicowe, ale określa tweety progresywne jako \"agresywne\" lub \"dzielące\".\"\"\"\n",
    "\n",
    "far_right_RU = \"\"\"Имя: Марк Томпсон\n",
    "Возраст: 52\n",
    "Пол: Мужчина\n",
    "Национальность: Русский\n",
    "Политическая группа: Крайне правый консерватор\n",
    "Взгляды: Не доверяет основным СМИ, считает, что «культура отмены» подавляет свободу слова. Рассматривает критику правой политики как несправедливые нападки. Склонен преуменьшать оскорбительность правых твитов, но называть прогрессивные твиты „агрессивными“ или «вызывающими разногласия».\"\"\"\n",
    "\n",
    "mod_cons_EN = \"\"\"Name: Linda Carter\n",
    "Age: 38\n",
    "Sex: Female\n",
    "Nationality: American\n",
    "Political Group: Moderate conservative\n",
    "Outlook: Values civility and tradition, dislikes extreme rhetoric from both sides. Might label overtly vulgar or conspiratorial tweets as offensive, even from the right.\"\"\"\n",
    "\n",
    "mod_cons_PL = \"\"\"Imię i nazwisko: Linda Carter\n",
    "Wiek: 38\n",
    "Płeć: Kobieta\n",
    "Nacjonalność: Polka\n",
    "Grupa polityczna: Umiarkowany konserwatysta\n",
    "Poglądy: Ceni uprzejmość i tradycję, nie lubi ekstremalnej retoryki z obu stron. Może uznać wulgarne lub konspiracyjne tweety za obraźliwe, nawet ze strony prawicy.\"\"\"\n",
    "\n",
    "mod_cons_RU = \"\"\"Имя: Линда Картер\n",
    "Возраст: 38\n",
    "Пол: Женщина\n",
    "Национальность: Русская\n",
    "Политическая группа: Умеренно-консервативная\n",
    "Взгляды: Ценит цивилизованность и традиции, не любит экстремальную риторику с обеих сторон. Может счесть оскорбительными откровенно вульгарные или конспирологические твиты, даже от правых.\"\"\"\n",
    "\n",
    "prog_left_EN = \"\"\"Name: Aisha Patel\n",
    "Age: 29\n",
    "Sex: Female\n",
    "Nationality: American\n",
    "Political Group: Progressive left\n",
    "Outlook: Highly sensitive to language targeting marginalized groups (racism, sexism). Likely to label tweets using terms like \"socialist\" pejoratively as offensive. Tolerant of aggressive progressive rhetoric if framed as social justice.\"\"\"\n",
    "\n",
    "prog_left_PL = \"\"\"Imię i nazwisko: Aisha Patel\n",
    "Wiek: 29 lat\n",
    "Płeć: Kobieta\n",
    "Nacjonalność: Polka\n",
    "Grupa polityczna: Postępowa lewica\n",
    "Perspektywy: Bardzo wrażliwa na język skierowany do grup marginalizowanych (rasizm, seksizm). Prawdopodobnie uzna za obraźliwe tweety używające pejoratywnych określeń takich jak „socjalista”. Toleruje agresywną, progresywną retorykę, jeśli jest oprawiona w ramy sprawiedliwości społecznej.\"\"\"\n",
    "\n",
    "prog_left_RU = \"\"\"Имя: Айша Патель\n",
    "Возраст: 29\n",
    "Пол: Женщина\n",
    "Национальность: Русская\n",
    "Политическая группа: Прогрессивные левые\n",
    "Взгляды: Очень чувствительна к языку, направленному на маргинализированные группы (расизм, сексизм). Скорее всего, сочтет оскорбительными твиты использующие уничижительные термины вроде «социалист». Терпимо относится к агрессивной прогрессивной риторике, если она оформлена как социальная справедливость.\"\"\"\n",
    "\n",
    "centrist_EN = \"\"\"Name: James Cooper\n",
    "Age: 45\n",
    "Sex: Male\n",
    "Nationality: American\n",
    "Political Group: Centrist/Independent\n",
    "Outlook: Pragmatic, dislikes partisan extremes. Harsher on tweets with misinformation or personal attacks, regardless of side. Less influenced by political loyalty; more likely to align annotations with factual accuracy and tone.\"\"\"\n",
    "\n",
    "centrist_PL = \"\"\"Imię i nazwisko: James Cooper\n",
    "Wiek: 45\n",
    "Płeć: Mężczyzna\n",
    "Nacjonalność: Polak\n",
    "Grupa polityczna: Centrysta/Niezależny\n",
    "Poglądy: Pragmatyczny, nie lubi skrajnych stronnictw. Ostrzejszy wobec tweetów zawierających dezinformacje lub ataki personalne, niezależnie od strony. Mniejszy wpływ lojalności politycznej; większe prawdopodobieństwo dostosowania adnotacji do faktycznej dokładności i tonu.\"\"\"\n",
    "\n",
    "centrist_RU = \"\"\"Имя: Джеймс Купер\n",
    "Возраст: 45\n",
    "Пол: Мужчина\n",
    "Национальность: Русский\n",
    "Политическая группа: Центрист/Независимый\n",
    "Взгляды: Прагматик, не любит крайностей в политике. Более жестко относится к твитам с дезинформацией или личными нападками, независимо от стороны. Менее подвержен влиянию политической лояльности; более склонен согласовывать аннотации с точностью фактов и тоном.\"\"\"\n",
    "system_prompt_EN = \"\"\"Give the answer as only 0(not offensive) or 1(offensive). Do not include anything else in your response.\"\"\"\n",
    "\n",
    "user_text_EN = \"\"\"You are a person of this personality: \n",
    "{personality}\n",
    "\n",
    "How this person will label that statement (offensive or not):\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "system_prompt_PL = \"\"\"Podaj odpowiedź jako 0 (nie obraźliwe) lub 1 (obraźliwe). Nie uwzględniaj niczego innego w swojej odpowiedzi.\"\"\"\n",
    "\n",
    "user_text_PL = \"\"\"Jesteś osobą o tej osobowości:\n",
    "{personality}\n",
    "\n",
    "Jak ta osoba określi to stwierdzenie (obraźliwe lub nie): Jesteś osobą o tej osobowości:\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "system_prompt_RU = \"\"\"Укажите в ответе только 0 (не оскорбительно) или 1 (оскорбительно). Не указывай в ответе ничего другого.\"\"\"\n",
    "\n",
    "user_text_RU = \"\"\"Ты - человек этой личности:\n",
    "{personality}\n",
    "\n",
    "Как этот человек обозначит это высказывание (оскорбительное или нет):\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "# Define personalities\n",
    "personalities = {\n",
    "    \"far_right_EN\": far_right_EN,\n",
    "    \"far_right_PL\": far_right_PL,\n",
    "    \"far_right_RU\": far_right_RU,\n",
    "    \"mod_cons_EN\": mod_cons_EN,\n",
    "    \"mod_cons_PL\": mod_cons_PL,\n",
    "    \"mod_cons_RU\": mod_cons_RU,\n",
    "    \"prog_left_EN\": prog_left_EN,\n",
    "    \"prog_left_PL\": prog_left_PL,\n",
    "    \"prog_left_RU\": prog_left_RU,\n",
    "    \"centrist_EN\": centrist_EN,\n",
    "    \"centrist_PL\": centrist_PL,\n",
    "    \"centrist_RU\": centrist_RU,\n",
    "}\n",
    "\n",
    "sys_prompt_mapping = {\n",
    "    \"system_prompt_EN\": system_prompt_EN,\n",
    "    \"system_prompt_PL\": system_prompt_PL,\n",
    "    \"system_prompt_RU\": system_prompt_RU\n",
    "}\n",
    "\n",
    "user_prompt_mapping = {\n",
    "    \"user_text_EN\": user_text_EN,\n",
    "    \"user_text_PL\": user_text_PL,\n",
    "    \"user_text_RU\": user_text_RU\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5970977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [01:17<00:00, 25.70s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-4B\", device_map='auto', token=token_hf)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\", device_map='auto', token=token_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90469a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get token probabilities from Qwen model in batches\n",
    "def get_qwen_probabilities(system_prompts, user_prompts, tokenizer, model, temperature=1):\n",
    "    \"\"\"\n",
    "    Process multiple conversations in parallel with the Qwen model,\n",
    "    returning probabilities for tokens \"0\" and \"1\".\n",
    "    \n",
    "    Args:\n",
    "        system_prompts: List of system prompts\n",
    "        user_prompts: List of user prompts\n",
    "        tokenizer: The tokenizer for the model\n",
    "        model: The Qwen model\n",
    "        temperature: Sampling temperature\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        A list of probability strings in format \"prob_for_1, prob_for_0\"\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(system_prompts) != len(user_prompts):\n",
    "        raise ValueError(\"Number of system prompts must match number of user prompts\")\n",
    "        \n",
    "    \n",
    "    # Token IDs for Qwen (based on tokenizer results)\n",
    "    # Qwen uses simple token IDs for digits\n",
    "    token_id_0 = 15  # Direct lookup token ID for \"0\"\n",
    "    token_id_1 = 16  # Direct lookup token ID for \"1\"\n",
    "    \n",
    "    # Process each conversation\n",
    "    probabilities = []\n",
    "    \n",
    "    for system_prompt, user_prompt in zip(system_prompts, user_prompts):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False  # Switches between thinking and non-thinking modes\n",
    "        )\n",
    "        \n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Forward pass through the model to get logits\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**model_inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get probabilities for the next token (last position in sequence)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                \n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Extract probabilities for \"0\" and \"1\" tokens\n",
    "            prob_0 = probs[token_id_0].item()\n",
    "            prob_1 = probs[token_id_1].item()\n",
    "        \n",
    "        # Format as requested: \"prob_for_1, prob_for_0\"\n",
    "        probability_str = f\"{prob_1:.4f}, {prob_0:.4f}\"\n",
    "        probabilities.append(probability_str)\n",
    "    \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d0df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data using the batch-enabled Qwen probability function\n",
    "def process_data_with_qwen(data, personalities, user_prompt_mapping, sys_prompt_mapping, tokenizer, model, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process data using the get_qwen_probabilities function in batches.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the data\n",
    "        personalities: Dictionary of personality prompts\n",
    "        user_prompt_mapping: Dictionary mapping language codes to user prompt templates\n",
    "        sys_prompt_mapping: Dictionary mapping language codes to system prompts\n",
    "        tokenizer: The tokenizer for the model\n",
    "        model: The Qwen model\n",
    "        batch_size: Number of requests to batch together\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with Qwen token probabilities\n",
    "    \"\"\"\n",
    "    # Ensure all columns exist with appropriate types\n",
    "    for language_code in ['EN', 'PL', 'RU']:\n",
    "        for personality_type in [\"far_right\", \"mod_cons\", \"prog_left\", \"centrist\"]:\n",
    "            answer_col = f\"Qwen_{personality_type}_{language_code}_answer\"\n",
    "            \n",
    "            # Make sure answer column exists\n",
    "            if answer_col not in data.columns:\n",
    "                data[answer_col] = pd.NA\n",
    "    \n",
    "    # Create a list to track all items that need processing\n",
    "    items_to_process = []\n",
    "    \n",
    "    # First scan to identify all items needing processing\n",
    "    for i, row in data.iterrows():\n",
    "        for language_code in ['EN', 'PL', 'RU']:\n",
    "            for personality_type in [\"far_right\", \"mod_cons\", \"prog_left\", \"centrist\"]:\n",
    "                personality_key = f\"{personality_type}_{language_code}\"\n",
    "                answer_col = f\"Qwen_{personality_type}_{language_code}_answer\"\n",
    "                \n",
    "                # Only process if answer column is empty\n",
    "                if pd.isna(data.loc[i, answer_col]):\n",
    "                    tweet = row[f\"Text_{language_code}\"]\n",
    "                    personality = personalities[personality_key]\n",
    "                    prompt = user_prompt_mapping[f\"user_text_{language_code}\"].format(personality=personality, tweet=tweet)\n",
    "                    \n",
    "                    items_to_process.append({\n",
    "                        'row_index': i,\n",
    "                        'personality_type': personality_type,\n",
    "                        'language_code': language_code,\n",
    "                        'system_prompt': sys_prompt_mapping[f\"system_prompt_{language_code}\"],\n",
    "                        'user_prompt': prompt\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(items_to_process)} items to process\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, len(items_to_process), batch_size)):\n",
    "        batch = items_to_process[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        # Extract system prompts and user prompts for the batch\n",
    "        system_prompts = [item['system_prompt'] for item in batch]\n",
    "        user_prompts = [item['user_prompt'] for item in batch]\n",
    "        \n",
    "        # Get probabilities\n",
    "        try:\n",
    "            probability_strs = get_qwen_probabilities(\n",
    "                system_prompts, \n",
    "                user_prompts,\n",
    "                tokenizer,\n",
    "                model,\n",
    "                temperature=1,\n",
    "            )\n",
    "            \n",
    "            # Update dataframe with results\n",
    "            for i, (item, prob_str) in enumerate(zip(batch, probability_strs)):\n",
    "                row_idx = item['row_index']\n",
    "                \n",
    "                # Store the probability string in the answer column\n",
    "                answer_col = f\"Qwen_{item['personality_type']}_{item['language_code']}_answer\"\n",
    "                data.loc[row_idx, answer_col] = prob_str\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {batch_start}: {e}\")\n",
    "        \n",
    "        # Save progress periodically with error handling\n",
    "        if batch_start % (batch_size * 5) == 0 or batch_start + batch_size >= len(items_to_process):\n",
    "            try:\n",
    "                # Create a copy of the dataframe with consistent types for saving\n",
    "                save_df = data.copy()\n",
    "                save_df.to_parquet(\"data/Qwen/data_qwen_progress.parquet\")\n",
    "                print(f\"Saved progress after processing {batch_start + len(batch)} items\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving data: {e}\")\n",
    "                # Try alternate save approach\n",
    "                try:\n",
    "                    # Save as TSV if parquet fails\n",
    "                    data.to_csv(\"data/Qwen/data_qwen_progress_backup.tsv\", sep=\"\\t\")\n",
    "                    print(\"Saved backup as TSV instead\")\n",
    "                except:\n",
    "                    print(\"Unable to save progress in any format. Continuing.\")\n",
    "            \n",
    "        # Optional: Add a small delay between batches to prevent overloading\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c634f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 items to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9999, 0.0001' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9990, 0.0010' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9997, 0.0003' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9992, 0.0008' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.0000, 0.0000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.0000, 0.0000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.8642, 0.1358' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1.0000, 0.0000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9956, 0.0044' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.7891, 0.2109' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0507, 0.9493' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_7915/2439601763.py:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.8553, 0.1447' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "  5%|▌         | 1/20 [00:11<03:36, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:09<02:42, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 1080 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:05<01:42, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 1980 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:03<00:45, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 2880 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:47<00:00, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 3600 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = process_data_with_qwen(\n",
    "    data, \n",
    "    personalities, \n",
    "    user_prompt_mapping, \n",
    "    sys_prompt_mapping,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    batch_size=180  # Adjust based on GPU memory\n",
    ")\n",
    "try:\n",
    "    # Create a copy of the dataframe with consistent types for saving\n",
    "    save_df = results.copy()\n",
    "    save_df.to_parquet(\"data/Qwen/data_qwen_complete.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "    # Try alternate save approach\n",
    "    try:\n",
    "        # Save as CSV if parquet fails\n",
    "        results.to_csv(\"data/Qwen/data_qwen_complete_backup.tsv\", sep='\\t')\n",
    "        print(\"Saved backup as TSV instead\")\n",
    "    except:\n",
    "        print(\"Unable to save complete in any format. Continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54fe0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Mean P(1)  Mean P(0)  Min P(1)  Max P(1)  Std P(1)  \\\n",
      "Qwen_far_right_EN_answer   0.994481   0.005519    0.0033       1.0  0.058946   \n",
      "Qwen_far_right_PL_answer   0.982371   0.017629    0.0005       1.0  0.117582   \n",
      "Qwen_far_right_RU_answer   0.974188   0.025812    0.0097       1.0  0.105389   \n",
      "Qwen_mod_cons_EN_answer    0.982216   0.017784    0.0687       1.0  0.080849   \n",
      "Qwen_mod_cons_PL_answer    0.879302   0.120698    0.0000       1.0  0.289853   \n",
      "Qwen_mod_cons_RU_answer    0.836830   0.163170    0.0008       1.0  0.299666   \n",
      "Qwen_prog_left_EN_answer   0.928080   0.071920    0.0601       1.0  0.163938   \n",
      "Qwen_prog_left_PL_answer   0.743984   0.256016    0.0000       1.0  0.408585   \n",
      "Qwen_prog_left_RU_answer   0.641199   0.358801    0.0019       1.0  0.360652   \n",
      "Qwen_centrist_EN_answer    0.987793   0.012207    0.0338       1.0  0.084605   \n",
      "Qwen_centrist_PL_answer    0.893765   0.106235    0.0000       1.0  0.285108   \n",
      "Qwen_centrist_RU_answer    0.892102   0.107898    0.0022       1.0  0.229464   \n",
      "\n",
      "                          Sanity Check (P1 + P0 ≈ 1)  \n",
      "Qwen_far_right_EN_answer                       False  \n",
      "Qwen_far_right_PL_answer                        True  \n",
      "Qwen_far_right_RU_answer                        True  \n",
      "Qwen_mod_cons_EN_answer                         True  \n",
      "Qwen_mod_cons_PL_answer                         True  \n",
      "Qwen_mod_cons_RU_answer                         True  \n",
      "Qwen_prog_left_EN_answer                        True  \n",
      "Qwen_prog_left_PL_answer                        True  \n",
      "Qwen_prog_left_RU_answer                       False  \n",
      "Qwen_centrist_EN_answer                         True  \n",
      "Qwen_centrist_PL_answer                         True  \n",
      "Qwen_centrist_RU_answer                         True  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7915/3857150553.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  P1 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[0].strip()))\n",
      "/tmp/ipykernel_7915/3857150553.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  P0 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[1].strip()))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split into two new DataFrames: P1 (prob. of 1) and P0 (prob. of 0)\n",
    "P1 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[0].strip()))\n",
    "P0 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[1].strip()))\n",
    "\n",
    "# Step 2: Analyze each column\n",
    "analysis = pd.DataFrame(index=results.iloc[:, 4:].columns)\n",
    "analysis['Mean P(1)'] = P1.mean()\n",
    "analysis['Mean P(0)'] = P0.mean()\n",
    "analysis['Min P(1)'] = P1.min()\n",
    "analysis['Max P(1)'] = P1.max()\n",
    "analysis['Std P(1)'] = P1.std()\n",
    "analysis['Sanity Check (P1 + P0 ≈ 1)'] = (P1 + P0).apply(lambda col: col.sub(1).abs().max() < 1e-6)\n",
    "\n",
    "print(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
