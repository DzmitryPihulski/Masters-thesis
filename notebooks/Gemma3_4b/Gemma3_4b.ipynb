{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e81bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "import requests\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "token_hf = os.environ['TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a501a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "far_right_EN = \"\"\"Name: Mark Thompson\n",
    "Age: 52\n",
    "Sex: Male\n",
    "Nationality: American\n",
    "Political Group: Far-right conservative\n",
    "Outlook: Distrusts mainstream media, believes \"cancel culture\" suppresses free speech. Views criticism of Right policies as unfair attacks. Likely to downplay offensiveness in right-wing tweets, but label progressive tweets as 'aggressive' or 'divisive.'\"\"\"\n",
    "\n",
    "far_right_PL = \"\"\"Imię i nazwisko: Mark Thompson\n",
    "Wiek: 52\n",
    "Płeć: Mężczyzna\n",
    "Nacjonalność: Polak\n",
    "Grupa polityczna: Skrajnie prawicowy konserwatysta\n",
    "Poglądy: Nie ufa mediom głównego nurtu, uważa, że „kultura anulowania” tłumi wolność słowa. Postrzega krytykę polityki prawicowej jako niesprawiedliwe ataki. Prawdopodobnie bagatelizuje obraźliwe tweety prawicowe, ale określa tweety progresywne jako \"agresywne\" lub \"dzielące\".\"\"\"\n",
    "\n",
    "far_right_RU = \"\"\"Имя: Марк Томпсон\n",
    "Возраст: 52\n",
    "Пол: Мужчина\n",
    "Национальность: Русский\n",
    "Политическая группа: Крайне правый консерватор\n",
    "Взгляды: Не доверяет основным СМИ, считает, что «культура отмены» подавляет свободу слова. Рассматривает критику правой политики как несправедливые нападки. Склонен преуменьшать оскорбительность правых твитов, но называть прогрессивные твиты „агрессивными“ или «вызывающими разногласия».\"\"\"\n",
    "\n",
    "mod_cons_EN = \"\"\"Name: Linda Carter\n",
    "Age: 38\n",
    "Sex: Female\n",
    "Nationality: American\n",
    "Political Group: Moderate conservative\n",
    "Outlook: Values civility and tradition, dislikes extreme rhetoric from both sides. Might label overtly vulgar or conspiratorial tweets as offensive, even from the right.\"\"\"\n",
    "\n",
    "mod_cons_PL = \"\"\"Imię i nazwisko: Linda Carter\n",
    "Wiek: 38\n",
    "Płeć: Kobieta\n",
    "Nacjonalność: Polka\n",
    "Grupa polityczna: Umiarkowany konserwatysta\n",
    "Poglądy: Ceni uprzejmość i tradycję, nie lubi ekstremalnej retoryki z obu stron. Może uznać wulgarne lub konspiracyjne tweety za obraźliwe, nawet ze strony prawicy.\"\"\"\n",
    "\n",
    "mod_cons_RU = \"\"\"Имя: Линда Картер\n",
    "Возраст: 38\n",
    "Пол: Женщина\n",
    "Национальность: Русская\n",
    "Политическая группа: Умеренно-консервативная\n",
    "Взгляды: Ценит цивилизованность и традиции, не любит экстремальную риторику с обеих сторон. Может счесть оскорбительными откровенно вульгарные или конспирологические твиты, даже от правых.\"\"\"\n",
    "\n",
    "prog_left_EN = \"\"\"Name: Aisha Patel\n",
    "Age: 29\n",
    "Sex: Female\n",
    "Nationality: American\n",
    "Political Group: Progressive left\n",
    "Outlook: Highly sensitive to language targeting marginalized groups (racism, sexism). Likely to label tweets using terms like \"socialist\" pejoratively as offensive. Tolerant of aggressive progressive rhetoric if framed as social justice.\"\"\"\n",
    "\n",
    "prog_left_PL = \"\"\"Imię i nazwisko: Aisha Patel\n",
    "Wiek: 29 lat\n",
    "Płeć: Kobieta\n",
    "Nacjonalność: Polka\n",
    "Grupa polityczna: Postępowa lewica\n",
    "Perspektywy: Bardzo wrażliwa na język skierowany do grup marginalizowanych (rasizm, seksizm). Prawdopodobnie uzna za obraźliwe tweety używające pejoratywnych określeń takich jak „socjalista”. Toleruje agresywną, progresywną retorykę, jeśli jest oprawiona w ramy sprawiedliwości społecznej.\"\"\"\n",
    "\n",
    "prog_left_RU = \"\"\"Имя: Айша Патель\n",
    "Возраст: 29\n",
    "Пол: Женщина\n",
    "Национальность: Русская\n",
    "Политическая группа: Прогрессивные левые\n",
    "Взгляды: Очень чувствительна к языку, направленному на маргинализированные группы (расизм, сексизм). Скорее всего, сочтет оскорбительными твиты использующие уничижительные термины вроде «социалист». Терпимо относится к агрессивной прогрессивной риторике, если она оформлена как социальная справедливость.\"\"\"\n",
    "\n",
    "centrist_EN = \"\"\"Name: James Cooper\n",
    "Age: 45\n",
    "Sex: Male\n",
    "Nationality: American\n",
    "Political Group: Centrist/Independent\n",
    "Outlook: Pragmatic, dislikes partisan extremes. Harsher on tweets with misinformation or personal attacks, regardless of side. Less influenced by political loyalty; more likely to align annotations with factual accuracy and tone.\"\"\"\n",
    "\n",
    "centrist_PL = \"\"\"Imię i nazwisko: James Cooper\n",
    "Wiek: 45\n",
    "Płeć: Mężczyzna\n",
    "Nacjonalność: Polak\n",
    "Grupa polityczna: Centrysta/Niezależny\n",
    "Poglądy: Pragmatyczny, nie lubi skrajnych stronnictw. Ostrzejszy wobec tweetów zawierających dezinformacje lub ataki personalne, niezależnie od strony. Mniejszy wpływ lojalności politycznej; większe prawdopodobieństwo dostosowania adnotacji do faktycznej dokładności i tonu.\"\"\"\n",
    "\n",
    "centrist_RU = \"\"\"Имя: Джеймс Купер\n",
    "Возраст: 45\n",
    "Пол: Мужчина\n",
    "Национальность: Русский\n",
    "Политическая группа: Центрист/Независимый\n",
    "Взгляды: Прагматик, не любит крайностей в политике. Более жестко относится к твитам с дезинформацией или личными нападками, независимо от стороны. Менее подвержен влиянию политической лояльности; более склонен согласовывать аннотации с точностью фактов и тоном.\"\"\"\n",
    "system_prompt_EN = \"\"\"Give the answer as only 0(not offensive) or 1(offensive). Do not include anything else in your response.\"\"\"\n",
    "\n",
    "user_text_EN = \"\"\"You are a person of this personality: \n",
    "{personality}\n",
    "\n",
    "How this person will label that statement (offensive or not):\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "system_prompt_PL = \"\"\"Podaj odpowiedź jako 0 (nie obraźliwe) lub 1 (obraźliwe). Nie uwzględniaj niczego innego w swojej odpowiedzi.\"\"\"\n",
    "\n",
    "user_text_PL = \"\"\"Jesteś osobą o tej osobowości:\n",
    "{personality}\n",
    "\n",
    "Jak ta osoba określi to stwierdzenie (obraźliwe lub nie): Jesteś osobą o tej osobowości:\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "system_prompt_RU = \"\"\"Укажите в ответе только 0 (не оскорбительно) или 1 (оскорбительно). Не указывай в ответе ничего другого.\"\"\"\n",
    "\n",
    "user_text_RU = \"\"\"Ты - человек этой личности:\n",
    "{personality}\n",
    "\n",
    "Как этот человек обозначит это высказывание (оскорбительное или нет):\n",
    "```\n",
    "{tweet}\n",
    "```\"\"\"\n",
    "\n",
    "# Define personalities\n",
    "personalities = {\n",
    "    \"far_right_EN\": far_right_EN,\n",
    "    \"far_right_PL\": far_right_PL,\n",
    "    \"far_right_RU\": far_right_RU,\n",
    "    \"mod_cons_EN\": mod_cons_EN,\n",
    "    \"mod_cons_PL\": mod_cons_PL,\n",
    "    \"mod_cons_RU\": mod_cons_RU,\n",
    "    \"prog_left_EN\": prog_left_EN,\n",
    "    \"prog_left_PL\": prog_left_PL,\n",
    "    \"prog_left_RU\": prog_left_RU,\n",
    "    \"centrist_EN\": centrist_EN,\n",
    "    \"centrist_PL\": centrist_PL,\n",
    "    \"centrist_RU\": centrist_RU,\n",
    "}\n",
    "\n",
    "sys_prompt_mapping = {\n",
    "    \"system_prompt_EN\": system_prompt_EN,\n",
    "    \"system_prompt_PL\": system_prompt_PL,\n",
    "    \"system_prompt_RU\": system_prompt_RU\n",
    "}\n",
    "\n",
    "user_prompt_mapping = {\n",
    "    \"user_text_EN\": user_text_EN,\n",
    "    \"user_text_PL\": user_text_PL,\n",
    "    \"user_text_RU\": user_text_RU\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef20fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", token=token_hf\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=token_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de4a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 1 to 300\n",
      "Data columns (total 16 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Text_EN                    300 non-null    object \n",
      " 1   Text_PL                    300 non-null    object \n",
      " 2   Text_RU                    300 non-null    object \n",
      " 3   Translation_quality        300 non-null    float64\n",
      " 4   Gemma_far_right_EN_answer  0 non-null      float64\n",
      " 5   Gemma_far_right_PL_answer  0 non-null      float64\n",
      " 6   Gemma_far_right_RU_answer  0 non-null      float64\n",
      " 7   Gemma_mod_cons_EN_answer   0 non-null      float64\n",
      " 8   Gemma_mod_cons_PL_answer   0 non-null      float64\n",
      " 9   Gemma_mod_cons_RU_answer   0 non-null      float64\n",
      " 10  Gemma_prog_left_EN_answer  0 non-null      float64\n",
      " 11  Gemma_prog_left_PL_answer  0 non-null      float64\n",
      " 12  Gemma_prog_left_RU_answer  0 non-null      float64\n",
      " 13  Gemma_centrist_EN_answer   0 non-null      float64\n",
      " 14  Gemma_centrist_PL_answer   0 non-null      float64\n",
      " 15  Gemma_centrist_RU_answer   0 non-null      float64\n",
      "dtypes: float64(13), object(3)\n",
      "memory usage: 37.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet('data/Gemma/data.parquet')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9253e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get token probabilities from Gemma model in batches\n",
    "def get_gemma_probabilities(system_prompts, user_prompts, processor, model):\n",
    "    \"\"\"\n",
    "    Process multiple conversations in parallel with the Gemma model,\n",
    "    returning probabilities for tokens \"0\" and \"1\".\n",
    "    \n",
    "    Args:\n",
    "        system_prompts: List of system prompts\n",
    "        user_prompts: List of user prompts\n",
    "        processor: The processor/tokenizer for the model\n",
    "        model: The Gemma model\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        A list of probability strings in format \"prob_for_1, prob_for_0\"\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if len(system_prompts) != len(user_prompts):\n",
    "        raise ValueError(\"Number of system prompts must match number of user prompts\")\n",
    "        \n",
    "    # Set random seed for reproducibility\n",
    "    \n",
    "    # Token IDs for Gemma (based on tokenizer results)\n",
    "    token_id_0 = 236771  # Direct lookup token ID for \"0\"\n",
    "    token_id_1 = 236770  # Direct lookup token ID for \"1\"\n",
    "    \n",
    "    # Prepare all conversations\n",
    "    all_messages = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        for system_prompt, user_prompt in zip(system_prompts, user_prompts)\n",
    "    ]\n",
    "    \n",
    "    # Process each conversation\n",
    "    probabilities = []\n",
    "    \n",
    "    for messages in all_messages:\n",
    "        # Apply chat template\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages, \n",
    "            add_generation_prompt=True, \n",
    "            tokenize=True,\n",
    "            return_dict=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device, dtype=torch.bfloat16)\n",
    "        \n",
    "        # Forward pass through the model to get logits\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get probabilities for the next token (last position in sequence)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Extract probabilities for \"0\" and \"1\" tokens\n",
    "            prob_0 = probs[token_id_0].item()\n",
    "            prob_1 = probs[token_id_1].item()\n",
    "        \n",
    "        # Format as requested: \"prob_for_1, prob_for_0\"\n",
    "        probability_str = f\"{prob_1:.4f}, {prob_0:.4f}\"\n",
    "        probabilities.append(probability_str)\n",
    "    \n",
    "    return probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afbdb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata = process_data_with_gemma(\\n    data, \\n    personalities, \\n    user_prompt_mapping, \\n    sys_prompt_mapping,\\n    processor,\\n    model,\\n    batch_size=4  # Adjust based on GPU memory\\n)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data using the batch-enabled Gemma probability function\n",
    "def process_data_with_gemma(data, personalities, user_prompt_mapping, sys_prompt_mapping, processor, model, batch_size=4):\n",
    "    \"\"\"\n",
    "    Process data using the get_gemma_probabilities function in batches.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the data\n",
    "        personalities: Dictionary of personality prompts\n",
    "        user_prompt_mapping: Dictionary mapping language codes to user prompt templates\n",
    "        sys_prompt_mapping: Dictionary mapping language codes to system prompts\n",
    "        processor: The processor/tokenizer for the model\n",
    "        model: The Gemma model\n",
    "        batch_size: Number of requests to batch together\n",
    "    \n",
    "    Returns:\n",
    "        Updated DataFrame with Gemma token probabilities\n",
    "    \"\"\"\n",
    "    # Ensure all columns exist with appropriate types\n",
    "    for language_code in ['EN', 'PL', 'RU']:\n",
    "        for personality_type in [\"far_right\", \"mod_cons\", \"prog_left\", \"centrist\"]:\n",
    "            answer_col = f\"Gemma_{personality_type}_{language_code}_answer\"\n",
    "            \n",
    "            # Make sure answer column exists\n",
    "            if answer_col not in data.columns:\n",
    "                data[answer_col] = pd.NA\n",
    "    \n",
    "    # Create a list to track all items that need processing\n",
    "    items_to_process = []\n",
    "    \n",
    "    # First scan to identify all items needing processing\n",
    "    for i, row in data.iterrows():\n",
    "        for language_code in ['EN', 'PL', 'RU']:\n",
    "            for personality_type in [\"far_right\", \"mod_cons\", \"prog_left\", \"centrist\"]:\n",
    "                personality_key = f\"{personality_type}_{language_code}\"\n",
    "                answer_col = f\"Gemma_{personality_type}_{language_code}_answer\"\n",
    "                \n",
    "                # Only process if answer column is empty\n",
    "                if pd.isna(data.loc[i, answer_col]):\n",
    "                    tweet = row[f\"Text_{language_code}\"]\n",
    "                    personality = personalities[personality_key]\n",
    "                    prompt = user_prompt_mapping[f\"user_text_{language_code}\"].format(personality=personality, tweet=tweet)\n",
    "                    \n",
    "                    items_to_process.append({\n",
    "                        'row_index': i,\n",
    "                        'personality_type': personality_type,\n",
    "                        'language_code': language_code,\n",
    "                        'system_prompt': sys_prompt_mapping[f\"system_prompt_{language_code}\"],\n",
    "                        'user_prompt': prompt\n",
    "                    })\n",
    "    \n",
    "    print(f\"Found {len(items_to_process)} items to process\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in tqdm(range(0, len(items_to_process), batch_size)):\n",
    "        batch = items_to_process[batch_start:batch_start + batch_size]\n",
    "        \n",
    "        # Extract system prompts and user prompts for the batch\n",
    "        system_prompts = [item['system_prompt'] for item in batch]\n",
    "        user_prompts = [item['user_prompt'] for item in batch]\n",
    "        \n",
    "        # Get probabilities\n",
    "        try:\n",
    "            probability_strs = get_gemma_probabilities(\n",
    "                system_prompts, \n",
    "                user_prompts,\n",
    "                processor,\n",
    "                model,\n",
    "            )\n",
    "            \n",
    "            # Update dataframe with results\n",
    "            for i, (item, prob_str) in enumerate(zip(batch, probability_strs)):\n",
    "                row_idx = item['row_index']\n",
    "                \n",
    "                # Store the probability string in the answer column\n",
    "                answer_col = f\"Gemma_{item['personality_type']}_{item['language_code']}_answer\"\n",
    "                data.loc[row_idx, answer_col] = prob_str\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {batch_start}: {e}\")\n",
    "        \n",
    "        # Save progress periodically with error handling\n",
    "        if batch_start % (batch_size * 5) == 0 or batch_start + batch_size >= len(items_to_process):\n",
    "            try:\n",
    "                # Create a copy of the dataframe with consistent types for saving\n",
    "                save_df = data.copy()\n",
    "                save_df.to_parquet(\"data/Gemma/data_gemma_progress.parquet\")\n",
    "                print(f\"Saved progress after processing {batch_start + len(batch)} items\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving data: {e}\")\n",
    "                # Try alternate save approach\n",
    "                try:\n",
    "                    # Save as CSV if parquet fails\n",
    "                    data.to_csv(\"data/Gemma/data_gemma_progress_backup.tsv\", sep=\"\\t\")\n",
    "                    print(\"Saved backup as TSV instead\")\n",
    "                except:\n",
    "                    print(\"Unable to save progress in any format. Continuing.\")\n",
    "            \n",
    "        # Optional: Add a small delay between batches to prevent overloading\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "data = process_data_with_gemma(\n",
    "    data, \n",
    "    personalities, \n",
    "    user_prompt_mapping, \n",
    "    sys_prompt_mapping,\n",
    "    processor,\n",
    "    model,\n",
    "    batch_size=4  # Adjust based on GPU memory\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4430782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3600 items to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.1152, 0.8848' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.2172, 0.7828' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0450, 0.9550' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0013, 0.9987' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9997, 0.0003' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9994, 0.0006' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0028, 0.9972' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9967, 0.0033' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9992, 0.0008' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0028, 0.9972' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.4662, 0.5338' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "/tmp/ipykernel_14935/3438802214.py:76: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0121, 0.9879' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[row_idx, answer_col] = prob_str\n",
      "  5%|▌         | 1/20 [00:13<04:17, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 180 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:23<03:17, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 1080 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:32<02:04, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 1980 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:42<00:55, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 2880 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:37<00:00, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved progress after processing 3600 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = process_data_with_gemma(\n",
    "    data, \n",
    "    personalities, \n",
    "    user_prompt_mapping, \n",
    "    sys_prompt_mapping,\n",
    "    processor,\n",
    "    model,\n",
    "    batch_size=180  # Adjust based on GPU memory\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Create a copy of the dataframe with consistent types for saving\n",
    "    save_df = results.copy()\n",
    "    save_df.to_parquet(\"data/Gemma/data_qwen_complete.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "    # Try alternate save approach\n",
    "    try:\n",
    "        # Save as CSV if parquet fails\n",
    "        results.to_csv(\"data/Gemma/data_qwen_complete_backup.tsv\", sep='\\t')\n",
    "        print(\"Saved backup as TSV instead\")\n",
    "    except:\n",
    "        print(\"Unable to save complete in any format. Continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c82fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Mean P(1)  Mean P(0)  Min P(1)  Max P(1)  Std P(1)  \\\n",
      "Gemma_far_right_EN_answer   0.646223   0.353777    0.0001    1.0000  0.444108   \n",
      "Gemma_far_right_PL_answer   0.914615   0.085384    0.0016    1.0000  0.251575   \n",
      "Gemma_far_right_RU_answer   0.694761   0.305236    0.0006    0.9999  0.429723   \n",
      "Gemma_mod_cons_EN_answer    0.764271   0.235728    0.0002    1.0000  0.407719   \n",
      "Gemma_mod_cons_PL_answer    0.795078   0.204921    0.0004    1.0000  0.383204   \n",
      "Gemma_mod_cons_RU_answer    0.598543   0.401443    0.0002    1.0000  0.473233   \n",
      "Gemma_prog_left_EN_answer   0.781958   0.218041    0.0001    1.0000  0.394417   \n",
      "Gemma_prog_left_PL_answer   0.839429   0.160571    0.0002    1.0000  0.348964   \n",
      "Gemma_prog_left_RU_answer   0.876710   0.123276    0.0005    1.0000  0.306586   \n",
      "Gemma_centrist_EN_answer    0.626481   0.373518    0.0001    1.0000  0.469225   \n",
      "Gemma_centrist_PL_answer    0.647344   0.352655    0.0002    1.0000  0.465694   \n",
      "Gemma_centrist_RU_answer    0.548928   0.451065    0.0002    1.0000  0.475525   \n",
      "\n",
      "                           Sanity Check (P1 + P0 ≈ 1)  \n",
      "Gemma_far_right_EN_answer                       False  \n",
      "Gemma_far_right_PL_answer                       False  \n",
      "Gemma_far_right_RU_answer                       False  \n",
      "Gemma_mod_cons_EN_answer                        False  \n",
      "Gemma_mod_cons_PL_answer                        False  \n",
      "Gemma_mod_cons_RU_answer                        False  \n",
      "Gemma_prog_left_EN_answer                       False  \n",
      "Gemma_prog_left_PL_answer                        True  \n",
      "Gemma_prog_left_RU_answer                       False  \n",
      "Gemma_centrist_EN_answer                        False  \n",
      "Gemma_centrist_PL_answer                        False  \n",
      "Gemma_centrist_RU_answer                        False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14935/3857150553.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  P1 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[0].strip()))\n",
      "/tmp/ipykernel_14935/3857150553.py:3: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  P0 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[1].strip()))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split into two new DataFrames: P1 (prob. of 1) and P0 (prob. of 0)\n",
    "P1 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[0].strip()))\n",
    "P0 = results.iloc[:, 4:].applymap(lambda x: float(x.split(',')[1].strip()))\n",
    "\n",
    "# Step 2: Analyze each column\n",
    "analysis = pd.DataFrame(index=results.iloc[:, 4:].columns)\n",
    "analysis['Mean P(1)'] = P1.mean()\n",
    "analysis['Mean P(0)'] = P0.mean()\n",
    "analysis['Min P(1)'] = P1.min()\n",
    "analysis['Max P(1)'] = P1.max()\n",
    "analysis['Std P(1)'] = P1.std()\n",
    "analysis['Sanity Check (P1 + P0 ≈ 1)'] = (P1 + P0).apply(lambda col: col.sub(1).abs().max() < 1e-6)\n",
    "\n",
    "print(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
